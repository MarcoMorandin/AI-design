the correlation coefficient and derive its energy envelope. The upper part of Fig. 3 shows the first 30 ms of the normalized autocorrelation coefficients (ACCs) at various distances. We observed that the second peak in the ACC aligns closely with the ideal ITDG values from Table I for \(h_{s}{=}1.5\) m. To strengthen this representation capturing both the level and timing of the first reflection, we compute the short-term power of the ACC (stpACC). The stpACC is obtained by applying a Hann-windowed moving average (size: 8 samples) to the squared ACC coefficients. At 24 kHz sampling rate, this \(\sim\)0.3 ms window groups reflections from objects or surfaces within 10 cm of each other. The resulting stpACC features for spatialized speech signals are shown in the lower part of Fig. 3.

To leverage stpACC features for the 3D SELD task and allow concatenation with conventional SELD features (e.g., IVs and log-mel spectrograms), we represent them as 2D signals. This is achieved by computing the short-time autocorrelation function in the frequency domain as:

\[ACC(t,\tau)=\mathcal{F}_{f\rightarrow\tau}^{-1}(\mathbf{X}(t,f)\mathbf{X}^{*} (t,f)) \tag{3}\]

\[ACC^{\mathrm{norm}}(t,\tau)=\frac{ACC(t,\tau)}{\max_{\tau}(|ACC(t,\tau)|)}, \quad\forall t \tag{4}\]

where \(\mathbf{X}(t,f)\) is the STFT of the W channel, \((.)^{*}\) denotes complex conjugate, and \(\mathcal{F}_{f\rightarrow\tau}^{-1}\) the inverse FFT from the frequency \(f\) to the time-lag domain \(\tau\). We then normalize each time bin \(t\) so that \(ACC^{\mathrm{norm}}(t,0){=}1\), and convolve its square with an 8-sample Hann window to obtain \(stpACC(t,\tau)\).

## III Experiments

### _Model Architecture_

We evaluated the proposed distance features using a CNN-Conformer architecture, widely adopted for SELD [8, 31, 32]. It consists of a CNN encoder, a Conformer module [33], and feed-forward layers for 3D SELD predictions. The CNN encoder processes FOA-derived acoustic features, including IVs in log-mel domain (3 channels), log-mel spectrograms from FOA (4 channels), and the proposed distance features, DRR, D+R, or spACC, forming an input of shape \(C_{\mathrm{in}}{\times}T_{\mathrm{in}}{\times}F_{\mathrm{in}}\). Here, \(T_{\mathrm{in}}\) and \(F_{\mathrm{in}}\) represent temporal and frequency (or time-lag) bins, respectively, with \(C_{\mathrm{in}}{=}8\) for DRR and sptACC or \(C_{\mathrm{in}}{=}9\) for D+R. The CNN encoder comprises four convolutional blocks with residual connections, each containing two 3\(\times\)3 convolutional layers, BN, ReLU activation, and Avg pooling with a stride of 2, halving the temporal and frequency dimension at each block. The resulting tensor of shape \(512\times T_{\mathrm{in}}/16\times F_{\mathrm{in}}/16\) is reshaped and frequency Avg pooling is applied to achieve a \(T_{\mathrm{in}}/16\times 512\) embedding. \(T_{\mathrm{in}}\) is chosen so that \(T_{\mathrm{in}}/16\) matches the label frame rate (10 labels / sec). A Conformer module with four layers and eight attention heads processes this embedding, using depthwise convolutions with kernel size of 51. Finally, two feedforward layers predict multi-ACCDOA vectors, modeling up to \(N{=}3\) tracks [12]. As in previous 3D SELD works [11, 12], the model is trained using class-wise Auxiliary Duplicating Permutation Invariant Training (ADPIT) loss [6].

### _Dataset and Data Augmentation_

We conducted our experiments using the STARSS23 dataset [7], which, to our knowledge, is the only public dataset for 3D SELD. Other well-known benchmarks for SELD, such as STARSS22 [34] or TAU-NIGENS Spatial Sound Events 2020 and 2021 [2, 3], do not include distance labels. STARSS23 [7] consists of \(\sim\)7.5h of real spatial recordings of acoustic scenes, temporally and spatially annotated, with 13 event classes. In our experiments, we employed the FOA audio format. The event class, DOA, and distance labels are provided at a resolution of 100ms. The dataset includes directional interferes, i.e., non-target sounds that should not be detected. Our experiments were conducted on the development set, which comes with a predefined train-test split. We evaluated our models on the test partition of the development set. To increase the size of the training data and mitigate overfitting, we augment the dataset by a factor of 8 using the audio channel swap (ACS) data augmentation [31]. We pre-trained our models using the synthetic 3D SELD data provided by the organizers of the DCASE2024 Task 3 Challenge, which consists of 20h of simulated data generated with RIRs, following a methodology similar to that described in [35]. We applied ACS data augmentation during pre-training too. We observed that pre-training our models is crucial for understanding the impact of different input features on distance estimation. This is likely because the model gains additional prior knowledge

Fig. 3: Autocorrelation coefficient at varying distances (top). Short-term power of the autocorrelation (bottom).

Fig. 2: RIRs from the omnidirectional FOA channel of the SurrRoom 1.0 dataset [30] (“Pop_Recording_Studio” room) used to spatialize speech at different distances. Direct sound peaks are temporally aligned for comparison.

