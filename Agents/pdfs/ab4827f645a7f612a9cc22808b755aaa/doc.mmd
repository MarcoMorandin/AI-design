# Reverberation-based Features for Sound Event Localization and Detection with Distance Estimation

Davide Berghi, and Philip J. B. Jackson,

Research supported by EPSRC and BBC Prosperity Partnership AI4ME: Future Personalised Object-Based Media Experiences Delivered at Scale Anywhere EP/V038087. For the purpose of open access, the author has applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising. Data supporting this study are available from [https://zenodo.org/records/7880637](https://zenodo.org/records/7880637) and from [https://zenodo.org/records/10932241.D](https://zenodo.org/records/10932241.D). Berghi and P. J. B. Jackson are with the Centre for Vision, Speech, and Signal Processing, University of Surrey, Guildford, U.K. (e-mail: {davide.berghi, p.jackson}@surrey.ac.uk).

###### Abstract

Sound event localization and detection (SELD) involves predicting active sound event classes over time while estimating their positions. The localization subtask in SELD is usually treated as a direction of arrival estimation problem, ignoring source distance. Only recently, SELD was extended to 3D by incorporating distance estimation, enabling the prediction of sound event positions in 3D space (3D SELD). However, existing methods lack input features designed for distance estimation. We argue that reverberation encodes valuable information for this task. This paper introduces two novel feature formats for 3D SELD based on reverberation: one using direct-to-reverberant ratio (DRR) and another leveraging signal autocorrelation to provide the model with insights into early reflections. Pre-training on synthetic data improves relative distance error (RDE) and overall SELD score, with autocorrelation-based features reducing RDE by over 3 percentage points on the STARSS23 dataset. The code to extract the features is available at github.com/dberghi/SELD-distance-features

 Distance Estimation, Sound Event Localization and Detection, Sound Source Localization, Reverberation.

## I Introduction

Sound event localization and detection (SELD) [1] integrates two subtasks: sound event detection (SED) and sound source localization (SSL). Thus, it involves identifying active sound events at any given time frame while estimating their spatial positions. SELD systems are important in many practical applications, e.g., human-robot interaction, security, augmented reality, accessibility, safety, and immersive production. SELD gained significant attention following its inclusion as a task in the Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge. Recent advancements in SELD research have tackled increasingly complex challenges, such as detecting moving sound events [2], ignoring external interfering sounds [3], distinguishing simultaneous same-class events originating from different directions of arrival (DOAs) [4, 5, 6], and leveraging the visual modality to tackle SELD as a multimodal task [7, 8, 9, 10].

The localization aspect of SELD is traditionally framed as a direction of arrival estimation (DOAE) problem, predicting the azimuth and elevation of sound events. However, this overlooks source distance, a crucial factor in many applications. The DCASE 2024 challenge addressed this by introducing distance estimation (3D SELD) [11]. Krause _et al_. [12] proposed two methods to support distance estimation in 3D SELD. The first method extends the multi-activity-coupled Cartesian DOA (multi-ACCDOA) vectors [6] to include distance estimation. Multi-ACCDOA vectors are commonly used in SELD as they simultaneously predict the DOAs of sound events and encode their activity level in the vector length (0 for inactive and 1 for active events). In the extended version presented in [12], the model predicts, for each event class \(c\), track \(n\), and time frame \(t\), a 3-element DOA vector - the (_x, y, z_) coordinates on the unit sphere - along with a distance value \(D_{net}\in\langle 0,\infty\rangle\). This representation, referred to as the multi-activity-coupled Cartesian Distance and DOA (multi-ACCDOA) method, incorporates distance estimation into the original framework. The second method presented in [12] includes a separate output branch specifically for distance estimation. Alternatively, Hong _et al_. [13] proposed an approach where the model directly predicts the 3D positions of sound events in the form of (_x, y, z_) coordinates. This approach combines DOA and distance into a single representation, offering a unified localization framework. However, it requires an additional output branch to handle the SED subtask.

Selecting the right input features is crucial in designing a SELD system [14]. Commonly adopted features include log-mel spectrograms for the SED subtask, intensity vectors (IV) [4] for DOAE in first-order ambisonics (FOA) audio format, and generalized cross-correlation with phase transform (GCC-PHAT) [15] or SALSA-Lite [16] for microphone array (MIC) format. However, to the best of our knowledge, features specifically designed for distance estimation have not yet been explored within the context of 3D SELD. Useful information about the sound events distance is encoded in the acoustic reverberation [17, 18, 19, 20, 21, 22]. This paper proposes and evaluates two reverberation-based input feature extraction methods. The first uses the direct-to-reverberant ratio (DRR) as an indicator of the energy balance between direct and reverberant sound. The second leverages the autocorrelation function to extract information about early reflections and to estimate the initial time delay gap (ITDG), the interval between direct sound arrival and the first major reflection. Experiments on the STARSS23 dataset [7] demonstrate that incorporating these features alongside log-mel spectrograms and intensity vectors (IV) enhances distance estimation accuracy and overall SELD performance.

The main contributions of this paper are threefold: (1) we propose two methods for extracting reverberation-based input features to address distance estimation in 3D SELD; (2)we conduct a preliminary study showing that autocorrelation-based features capture distance-related information by analyzing how an audio clip interacts with room impulse responses (RIRs) recorded at different distances; (3) we validate the effectiveness of the proposed features on real data.

## II Proposed Reverberation-based Features

Log-mel spectrograms and intensity vectors (IVs) work well for SED and DOAE but are not suited for distance estimation. To address this, we introduce two input features specifically designed to enhance distance estimation.

### _Direct-to-Reverberant Features_

Distance cues can be extracted from the relationship between the direct and reverberant components of the captured audio signals [17]. To estimate the direct sound, \(d(t)\), we employed the Weighted Prediction Error (WPE) dereverberation algorithm [23] applied to the omnidirectional channel W of the first-order ambisonic (FOA) audio format. Specifically, we adopted the Python implementation of the WPE algorithm released by Drude _et al_. [24] (taps=60; delay=5; iterations=5). The reverberant component, \(r(t)\), is then estimated by subtracting the direct signal from the original signal in the temporal domain. To extract DRR features as 2D inputs to the model and to enable concatenation with the other SELD features (i.e., log-mel spectrograms and IVs), we calculate the DRR as a function of time and frequency, and then mapped it to log-mel space. The proposed DRR input features, \(\mathbf{DRR}^{\mathrm{mel}}\), are defined as:

\[\mathbf{DRR}^{\mathrm{mel}}(t,k)=10\cdot\log_{10}\left(\mathbf{P}_{\mathrm{ DRR}}^{\mathrm{mel}}(t,k)\right) \tag{1}\]

\[\mathbf{P}_{\mathrm{DRR}}^{\mathrm{mel}}(t,k)=\sum_{f=0}^{F}\mathbf{H}^{ \mathrm{mel}}(k,f)\left(\frac{\mathbf{P}_{\mathrm{D}}(t,f)}{\mathbf{P}_{ \mathrm{R}}(t,f)}\right) \tag{2}\]

where \(\mathbf{H}^{\mathrm{mel}}\) denotes the mel filter bank, which maps the frequency spectrum to the mel scale, with \(k\) being the mel bin index. \(\mathbf{P}_{\mathrm{D}}(t,f)\) and \(\mathbf{P}_{\mathrm{R}}(t,f)\) are the power spectral densities (PSDs) of the direct and reverberant components, respectively. To prevent instability and avoid division by zero, the PSD values were clamped to a small positive constant, \(\epsilon\)=\(1e{-}10\). Mathematically, \(\mathbf{P}_{\mathrm{D}}(f,t)\) and \(\mathbf{P}_{\mathrm{R}}(f,t)\) can be defined as \(\mathbf{P}_{\mathrm{D}}(t,f)\)=\(\max(|\mathbf{D}(t,f)|^{2},\epsilon)\) and \(\mathbf{P}_{\mathrm{R}}(t,f)\)=\(\max(|\mathbf{R}(t,f)|^{2},\epsilon)\), where \(\mathbf{D}(t,f)\) and \(\mathbf{R}(t,f)\) are the short-term Fourier transforms (STFTs) of the direct and reverberant components, \(d(t)\) and \(r(t)\), respectively.

In addition to the DRR features described, we explore a variant where \(\mathbf{D}(t,f)\) and \(\mathbf{R}(t,f)\) are separately converted into log-mel spectrograms and fed into the model. This approach, introduced in our DCASE2024 Task 3 submission [25], aims to give the network greater flexibility in learning task-relevant information. We refer to these features as D+R features.

### _Short-term Power of the Autocorrelation_

For the second feature, we explore the role of early reflections in distance estimation, focusing on the ITDG, a key cue for perceiving distance [26, 27]. While early reflection delays also depend on room size, it is reasonable to assume that the earliest reflection originates from the floor [27], as shown in Fig. 1. From this assumption, Table I demonstrates that ITDG from floor reflections decreases as the source-microphone distance increases. These values assume a microphone height of \(h_{m}\)=\(1.5\)m, as in the STARSS23 dataset [7], and a source height of \(h_{s}\)=\(0.9\)m, reflecting the average event height in the training set, similar to a seated user. We also include sources at 1.5m, representing standing speakers. Although these conditions may not always hold, we argue that the model can learn prior knowledge about typical source heights based on class. For instance, speech is unlikely to originate from the ceiling or floor, whereas footsteps are naturally associated with the ground. Ideally, the model should determine when and how to incorporate such priors to refine distance estimation.

To design an input feature that captures early reflections, we conducted a preliminary study on ITDG variations across different source distances. We analyzed an 8s speech clip from the S3A Object-based Audio Drama dataset [28, 29] and convolved it with room impulse responses (RIRs) from SurrRoom 1.0 [30]. Specifically, we used the omnidirectional W channel of FOA RIRs recorded in the "Pop_Recording_Studio" at distances of [1m, 1.5m, 2m, 2.5m, 3m].

Fig. 2 shows the aligned RIRs, where the first reflection, i.e., the initial peak after the direct sound, shifts closer to the direct sound as distance increases, consistent with Table I. A later strong reflection, likely from the rear wall, also appears, with increasing delay at greater distances. While wall reflections depend on room size and geometry, making them unreliable for distance estimation, floor reflections offer a more robust and consistent cue for this task.

After spatializing the clip at different distances, we compute

Fig. 1: Floor reflection path when source and receiver are at the same height (\(h_{s}\)=\(h_{m}\)) and separated by distance \(d\).

\begin{table}
\begin{tabular}{c||c|c|c||c|c|c} \cline{2-7}  & \multicolumn{3}{c||}{**Source Height: 1.5m**} & \multicolumn{3}{c}{**Source Height: 0.9m**} \\ \hline
**Dist** & **Direct** & **1stRef** & **ITDG** & **Direct** & **1stRef** & **ITDG** \\ \hline
1.0 m & 2.9 ms & 9.2 ms & 6.3 ms & 3.4 ms & 7.6 ms & 4.2 ms \\
1.5 m & 4.4 ms & 9.8 ms & 5.4 ms & 4.7 ms & 8.2 ms & 3.5 ms \\
2.0 m & 5.8 ms & 10.5 ms & 4.7 ms & 6.1 ms & 9.1 ms & 3.0 ms \\
2.5 m & 7.3 ms & 11.4 ms & 4.1 ms & 7.5 ms & 10.1 ms & 2.6 ms \\
3.0 m & 8.7 ms & 12.4 ms & 3.6 ms & 8.9 ms & 11.2 ms & 2.3 ms \\ \end{tabular}
\end{table} TABLE I: Ideal direct sound and first reflection (1stRef) delays, with their corresponding initial time delay gaps (ITDGs), as the source distance increases, assuming the first reflection originates from the floor. We present cases for source heights of 1.5 m and 0.9 m, with microphone positioned at 1.5 m, sound speed of 343 m/s, and no additional interfering factors.

the correlation coefficient and derive its energy envelope. The upper part of Fig. 3 shows the first 30 ms of the normalized autocorrelation coefficients (ACCs) at various distances. We observed that the second peak in the ACC aligns closely with the ideal ITDG values from Table I for \(h_{s}{=}1.5\) m. To strengthen this representation capturing both the level and timing of the first reflection, we compute the short-term power of the ACC (stpACC). The stpACC is obtained by applying a Hann-windowed moving average (size: 8 samples) to the squared ACC coefficients. At 24 kHz sampling rate, this \(\sim\)0.3 ms window groups reflections from objects or surfaces within 10 cm of each other. The resulting stpACC features for spatialized speech signals are shown in the lower part of Fig. 3.

To leverage stpACC features for the 3D SELD task and allow concatenation with conventional SELD features (e.g., IVs and log-mel spectrograms), we represent them as 2D signals. This is achieved by computing the short-time autocorrelation function in the frequency domain as:

\[ACC(t,\tau)=\mathcal{F}_{f\rightarrow\tau}^{-1}(\mathbf{X}(t,f)\mathbf{X}^{*} (t,f)) \tag{3}\]

\[ACC^{\mathrm{norm}}(t,\tau)=\frac{ACC(t,\tau)}{\max_{\tau}(|ACC(t,\tau)|)}, \quad\forall t \tag{4}\]

where \(\mathbf{X}(t,f)\) is the STFT of the W channel, \((.)^{*}\) denotes complex conjugate, and \(\mathcal{F}_{f\rightarrow\tau}^{-1}\) the inverse FFT from the frequency \(f\) to the time-lag domain \(\tau\). We then normalize each time bin \(t\) so that \(ACC^{\mathrm{norm}}(t,0){=}1\), and convolve its square with an 8-sample Hann window to obtain \(stpACC(t,\tau)\).

## III Experiments

### _Model Architecture_

We evaluated the proposed distance features using a CNN-Conformer architecture, widely adopted for SELD [8, 31, 32]. It consists of a CNN encoder, a Conformer module [33], and feed-forward layers for 3D SELD predictions. The CNN encoder processes FOA-derived acoustic features, including IVs in log-mel domain (3 channels), log-mel spectrograms from FOA (4 channels), and the proposed distance features, DRR, D+R, or spACC, forming an input of shape \(C_{\mathrm{in}}{\times}T_{\mathrm{in}}{\times}F_{\mathrm{in}}\). Here, \(T_{\mathrm{in}}\) and \(F_{\mathrm{in}}\) represent temporal and frequency (or time-lag) bins, respectively, with \(C_{\mathrm{in}}{=}8\) for DRR and sptACC or \(C_{\mathrm{in}}{=}9\) for D+R. The CNN encoder comprises four convolutional blocks with residual connections, each containing two 3\(\times\)3 convolutional layers, BN, ReLU activation, and Avg pooling with a stride of 2, halving the temporal and frequency dimension at each block. The resulting tensor of shape \(512\times T_{\mathrm{in}}/16\times F_{\mathrm{in}}/16\) is reshaped and frequency Avg pooling is applied to achieve a \(T_{\mathrm{in}}/16\times 512\) embedding. \(T_{\mathrm{in}}\) is chosen so that \(T_{\mathrm{in}}/16\) matches the label frame rate (10 labels / sec). A Conformer module with four layers and eight attention heads processes this embedding, using depthwise convolutions with kernel size of 51. Finally, two feedforward layers predict multi-ACCDOA vectors, modeling up to \(N{=}3\) tracks [12]. As in previous 3D SELD works [11, 12], the model is trained using class-wise Auxiliary Duplicating Permutation Invariant Training (ADPIT) loss [6].

### _Dataset and Data Augmentation_

We conducted our experiments using the STARSS23 dataset [7], which, to our knowledge, is the only public dataset for 3D SELD. Other well-known benchmarks for SELD, such as STARSS22 [34] or TAU-NIGENS Spatial Sound Events 2020 and 2021 [2, 3], do not include distance labels. STARSS23 [7] consists of \(\sim\)7.5h of real spatial recordings of acoustic scenes, temporally and spatially annotated, with 13 event classes. In our experiments, we employed the FOA audio format. The event class, DOA, and distance labels are provided at a resolution of 100ms. The dataset includes directional interferes, i.e., non-target sounds that should not be detected. Our experiments were conducted on the development set, which comes with a predefined train-test split. We evaluated our models on the test partition of the development set. To increase the size of the training data and mitigate overfitting, we augment the dataset by a factor of 8 using the audio channel swap (ACS) data augmentation [31]. We pre-trained our models using the synthetic 3D SELD data provided by the organizers of the DCASE2024 Task 3 Challenge, which consists of 20h of simulated data generated with RIRs, following a methodology similar to that described in [35]. We applied ACS data augmentation during pre-training too. We observed that pre-training our models is crucial for understanding the impact of different input features on distance estimation. This is likely because the model gains additional prior knowledge

Fig. 3: Autocorrelation coefficient at varying distances (top). Short-term power of the autocorrelation (bottom).

Fig. 2: RIRs from the omnidirectional FOA channel of the SurrRoom 1.0 dataset [30] (“Pop_Recording_Studio” room) used to spatialize speech at different distances. Direct sound peaks are temporally aligned for comparison.

about sound events, enabling a better interpretation of the information encoded in the input features.

### _Metrics_

To evaluate our models, we adopted the official metrics of the DCASE 2024 Task 3 Challenge [11] that are based on true positive (TP) and false positive (FP) predictions. A prediction is considered TP if the class prediction is correct and if its predicted DOA is within \(\pm 20^{\circ}\) from the target, and the relative distance error (\(RDE\)=\(|L_{p}-L_{r}|/L_{r}\) with \(L_{p}\) and \(L_{r}\) being the predicted and reference distance, respectively) is smaller than 1. Metrics are computed at the frame level and for each class independently and then averaged across the number of classes. Based on these, the adopted metrics are the class- and location-dependent F1 score (\(F_{\leq 20^{\circ}/1}\)), the class-dependent DOA error (\(DOAE\)), and the class-dependent relative distance error (\(RDE\)) [11]. We also include the \(SELD\) score that encodes the overall 3D SELD performance and is achieved as: \(SELD\)=\(\mathrm{mean}((1-F_{\leq 20^{\circ}/1}),DOAE/180,RDE)\).

### _Hyper-parameters and Experimental Settings_

We trained our models using 3-second audio chunks, extracted every 1 s for training and without overlap for testing. Spectrograms were generated via STFT with a 512-point Hann window and 150-sample hop size, yielding 480 temporal bins for 24kHz audio. Log-mel spectrograms (128 frequency bins) were computed for audio channels, DRR, D+R, and IV features. For stpACC features, we applied an STFT with a 1014-point Hann window. This ensures that, when considering only the positive time-lags \(\tau\)\(>\) 0, \(stpACC(t,\tau)\) contains 512 time-lag bins, covering delays up to approximately 20 ms after the direct sound. We then downsample the time-lag dimension by a factor of 4 to achieve 128 bins and allow concatenation with the other features. Models were trained with batch size 32 using Adam optimizer for 50 epochs, selecting the best based on the lowest SELD score. The learning rate was 5e-5 for 30 epochs, then reduced by 5% per epoch.

### _Results_

The results obtained using the proposed distance features are presented in Table II. Confidence intervals were estimated using the jackknife estimate of variance [36], applying the leave-one-out resampling technique to each of the 78 sequences in the test set. The table shows that all tested distance features contributed to a reduction in \(RDE\), leading to an overall improvement in \(SELD\) score. A small but not significant improvement was observed in \(F_{\leq 20^{\circ}/1}\). This is expected, as the distance features are specifically designed to enhance distance estimation, while the TP predictions used to compute \(F_{\leq 20^{\circ}/1}\) depend on an \(RDE\)\(<\)1 threshold, which is a relatively trivial condition to meet even without distance features.

The model without distance features achieved the lowest \(DOAE\). However, considering the confidence intervals, this difference appears to fall within statistical noise. Nevertheless, the \(DOAE\) confidence interval obtained with stpACC features is over three times larger than that of the other features, indicating greater variability and noisier estimates for this metric. Despite this, stpACC features also yielded the best \(RDE\) and the highest \(SELD\) score. While DRR is a known indicator in distance perception [19, 22, 37, 38], D+R features yielded better distance estimates. We hypothesize that the model benefits from greater flexibility in learning task-relevant information from the direct and reverberant components separately.

## IV Conclusion

This paper introduces novel distance input features for 3D SELD, leveraging reverberation cues encoded in the audio signal. The first category of proposed features separates direct and reverberant components, which are either fed to the model independently or represented as a direct-to-reverberant ratio. The second category aims to enhance the model's understanding of early reflections, particularly the first reflection.

Experiments on STARSS23 indicate that the proposed features improve distance estimation, leading to enhanced overall SELD score. The most significant improvement was observed with short-term power of the autocorrelation features. Future research will explore the effectiveness of reverberation-based distance features across different 3D SELD data, including synthetic data, and various network architectures.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline
**Distance Features** & \(F_{\leq 20^{\circ}/1}\) \(\uparrow\) & \(DOAE\) \(\downarrow\) & \(RDE\) \(\downarrow\) & \(SELD\) \(\downarrow\) \\ \hline None & 34.7\% (29.7\% - 39.3\%) & **19.4**\({}^{\circ}\) (16.6\({}^{\circ}\)-22.2\({}^{\circ}\)) & 0.296 (0.273 - 0.355) & 0.352 (0.332 - 0.385) \\ \hline D+R & **36.4\%** (31.1\% - 41.6\%) & 22.0\({}^{\circ}\) (18.6\({}^{\circ}\)-24.2\({}^{\circ}\)) & 0.273 (0.234 - 0.298) & 0.344 (0.314 - 0.367) \\ \hline DRR & 36.0\% (30.8\% - 41.4\%) & 20.1\({}^{\circ}\) (17.7\({}^{\circ}\)-23.4\({}^{\circ}\)) & 0.286 (0.240 - 0.315) & 0.346 (0.319 - 0.568) \\ \hline stpACC & 35.9\% (30.5\% - 41.2\%) & 21.3\({}^{\circ}\) (11.9\({}^{\circ}\)-30.6\({}^{\circ}\)) & **0.262** (0.225 - 0.296) & **0.341** (0.304 - 0.375) \\ \hline \end{tabular}
\end{table} TABLE II: Result with respective 95% confidence intervals achieved with the proposed distance input features. Each row represents a model trained using the concatenation of log-mel spectrograms, intensity vectors (IVs), and the different distance features. For the first row (“None”), only log-mel spectrograms and IVs are employed.

Fig. 4: Distance features with respective log mel spectrogram extracted from a sequence of STARSS23.



## References

* [1]S. Adavanne, A. Politis, J. Nikunen, and T. Vittanen (2019) Sound event localization and detection of overlapping sources using convolutional recurrent neural networks. IEEE Journal of Selected Topics in Signal Processing13, pp. 34-48. Cited by: SSI.
* [2]A. Politis, S. Adavanne, and T. Vittanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [3]A. Politis, S. Bech, S. J. Holdt, and T. van Waterschoot (2014) Perception of reverberation in small rooms: a literature study. In Audio Engineering Society Conference, Cited by: SSII.
* [4]A. Politis, S. Adavanne, D. Krause, A. Deleforge, P. Srivastava, and T. Virtanen (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [5]A. Politis, K. Shimada, P. Sudarsanam, D. Krause, Y. Koyama, N. Takahashi, Y. Mitsufuji, and T. Virtanen (2022) STARSS2: a dataset of spatial recordings of real scenes with smartphone feature for polyphonic sound event localization and detection with microphone arrays. In IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 716-720. Cited by: SSI.
* [6]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [7]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [8]A. Politis, K. Shimada, P. Sudarsanam, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [9]A. Politis, S. Adavanne, D. Krause, A. Deleforge, P. Srivastava, and T. Virtanen (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [10]A. Politis, K. Shimada, A. Politis, P. Sudarsanam, D. Krause, A. Deleforge, P. Srivastava, and T. Virtanen (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [11]A. Politis, K. Shimada, P. Sudarsanam, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [12]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [13]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [14]A. Politis, K. Shimada, P. Sudarsanam, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [15]A. Politis, S. Adavanne, D. Krause, A. Deleforge, P. Srivastava, and T. Virtanen (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [16]A. Politis, K. Shimada, P. Sudarsanam, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [17]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [18]A. Politis, S. Adavanne, D. Krause, A. Deleforge, P. Srivastava, and T. Virtanen (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [19]A. Politis, K. Shimada, P. Sudarsanam, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [20]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [21]A. Politis, S. Adavanne, D. Krause, A. Deleforge, P. Srivastava, and T. Virtanen (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [22]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [23]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [24]A. Politis, S. Adavanne, D. Krause, A. Deleforge, P. Srivastava, and T. Virtanen (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [25]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [26]A. Politis, S. Adavanne, D. Krause, A. Deleforge, P. Srivastava, and T. Virtanen (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [27]A. Politis, K. Shimada, P. Sudarsanam, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [28]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [29]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [30]A. Politis, S. Adavanne, D. Krause, A. Deleforge, P. Srivastava, and T. Virtanen (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [31]A. Politis, S. Adavanne, D. Krause, A. Deleforge, P. Srivastava, and T. Virtanen (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [32]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [33]A. Politis, S. Adavanne, D. Krause, A. Deleforge, P. Srivastava, and T. Virtanen (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [34]A. Politis, K. Shimada, P. Sudarsanam, D. Krause, A. Deleforge, P. Srivastava, and T. Virtanen (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [35]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [36]A. Politis, S. Adavanne, D. Krause, A. Deleforge, and P. Srivastava (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [37]A. Politis, K. Shimada, A. Politis, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [38]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [39]A. Politis, S. Adavanne, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [40]A. Politis, K. Shimada, P. Sudarsanam, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [41]A. Politis, K. Shimada, P. Sudarsanam, D. Krause, A. Deleforge, P. Srivastava, and T. Virtanen (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [42]A. Politis, K. Shimada, P. Sudarsanam, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [43]A. Politis, S. Adavanne, D. Krause, A. Deleforge, and P. Srivastava (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [44]A. Politis, K. Shimada, P. Sudarsanam, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [45]A. Politis, K. Shimada, P. Sudarsanam, D. Krause, A. Deleforge, and P. Srivastava (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [46]A. Politis, K. Shimada, P. Sudarsanam, D. Krause, A. Deleforge, and P. Srivastava (2021) A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [47]A. Politis, K. Shimada, A. Politis, and T. Virtanen (2020) A dataset of reverberant spatial sound scenes with moving sources for sound event localization and detection. In Detection and Classification of Acoustic Scenes and Events Workshop, Cited by: SSI.
* [48]A. Politis, K. Shimada, P