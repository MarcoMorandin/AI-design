

# Reverberation-based Features for Sound Event Localization and Detection with Distance Estimation

Davide Berghi, and Philip J. B. Jackson,

Research supported by EPSRC and BBC Prosperity Partnership AI4ME: Future Personalised Object-Based Media Experiences Delivered at Scale Anywhere EP/V038087. For the purpose of open access, the author has applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising. Data supporting this study are available from [https://zenodo.org/records/7880637](https://zenodo.org/records/7880637) and from [https://zenodo.org/records/10932241.D](https://zenodo.org/records/10932241.D). Berghi and P. J. B. Jackson are with the Centre for Vision, Speech, and Signal Processing, University of Surrey, Guildford, U.K. (e-mail: {davide.berghi, p.jackson}@surrey.ac.uk).

###### Abstract

Sound event localization and detection (SELD) involves predicting active sound event classes over time while estimating their positions. The localization subtask in SELD is usually treated as a direction of arrival estimation problem, ignoring source distance. Only recently, SELD was extended to 3D by incorporating distance estimation, enabling the prediction of sound event positions in 3D space (3D SELD). However, existing methods lack input features designed for distance estimation. We argue that reverberation encodes valuable information for this task. This paper introduces two novel feature formats for 3D SELD based on reverberation: one using direct-to-reverberant ratio (DRR) and another leveraging signal autocorrelation to provide the model with insights into early reflections. Pre-training on synthetic data improves relative distance error (RDE) and overall SELD score, with autocorrelation-based features reducing RDE by over 3 percentage points on the STARSS23 dataset. The code to extract the features is available at github.com/dberghi/SELD-distance-features

 Distance Estimation, Sound Event Localization and Detection, Sound Source Localization, Reverberation.

## I Introduction

Sound event localization and detection (SELD) [1] integrates two subtasks: sound event detection (SED) and sound source localization (SSL). Thus, it involves identifying active sound events at any given time frame while estimating their spatial positions. SELD systems are important in many practical applications, e.g., human-robot interaction, security, augmented reality, accessibility, safety, and immersive production. SELD gained significant attention following its inclusion as a task in the Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge. Recent advancements in SELD research have tackled increasingly complex challenges, such as detecting moving sound events [2], ignoring external interfering sounds [3], distinguishing simultaneous same-class events originating from different directions of arrival (DOAs) [4, 5, 6], and leveraging the visual modality to tackle SELD as a multimodal task [7, 8, 9, 10].

The localization aspect of SELD is traditionally framed as a direction of arrival estimation (DOAE) problem, predicting the azimuth and elevation of sound events. However, this overlooks source distance, a crucial factor in many applications. The DCASE 2024 challenge addressed this by introducing distance estimation (3D SELD) [11]. Krause _et al_. [12] proposed two methods to support distance estimation in 3D SELD. The first method extends the multi-activity-coupled Cartesian DOA (multi-ACCDOA) vectors [6] to include distance estimation. Multi-ACCDOA vectors are commonly used in SELD as they simultaneously predict the DOAs of sound events and encode their activity level in the vector length (0 for inactive and 1 for active events). In the extended version presented in [12], the model predicts, for each event class \(c\), track \(n\), and time frame \(t\), a 3-element DOA vector - the (_x, y, z_) coordinates on the unit sphere - along with a distance value \(D_{net}\in\langle 0,\infty\rangle\). This representation, referred to as the multi-activity-coupled Cartesian Distance and DOA (multi-ACCDOA) method, incorporates distance estimation into the original framework. The second method presented in [12] includes a separate output branch specifically for distance estimation. Alternatively, Hong _et al_. [13] proposed an approach where the model directly predicts the 3D positions of sound events in the form of (_x, y, z_) coordinates. This approach combines DOA and distance into a single representation, offering a unified localization framework. However, it requires an additional output branch to handle the SED subtask.

Selecting the right input features is crucial in designing a SELD system [14]. Commonly adopted features include log-mel spectrograms for the SED subtask, intensity vectors (IV) [4] for DOAE in first-order ambisonics (FOA) audio format, and generalized cross-correlation with phase transform (GCC-PHAT) [15] or SALSA-Lite [16] for microphone array (MIC) format. However, to the best of our knowledge, features specifically designed for distance estimation have not yet been explored within the context of 3D SELD. Useful information about the sound events distance is encoded in the acoustic reverberation [17, 18, 19, 20, 21, 22]. This paper proposes and evaluates two reverberation-based input feature extraction methods. The first uses the direct-to-reverberant ratio (DRR) as an indicator of the energy balance between direct and reverberant sound. The second leverages the autocorrelation function to extract information about early reflections and to estimate the initial time delay gap (ITDG), the interval between direct sound arrival and the first major reflection. Experiments on the STARSS23 dataset [7] demonstrate that incorporating these features alongside log-mel spectrograms and intensity vectors (IV) enhances distance estimation accuracy and overall SELD performance.

The main contributions of this paper are threefold: (1) we propose two methods for extracting reverberation-based input features to address distance estimation in 3D SELD; (2)