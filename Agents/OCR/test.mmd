# Categorical Unsupervised Variational Acoustic Clustering

Luan Vinicius Fiorio, Ivana Nikoloska, and Ronald M. Aarts

This work was supported by the Robust AI for Safe (radar) signal processing (RAIS) collaboration framework between Eindhoven University of Technology and NXP Semiconductors, including a Privaut-Publicke Samenwerkingen-toeslag (PPS) supplement from the Dutch Ministry of Economic Affairs and Climate Policy.L. V. Fiorio, I. Nikoloska, and R. M. Aarts are with the Eindhoven University of Technology, Eindhoven, 5612 AP, The Netherlands (e-mails: l.v.fiorio@tue.nl, inikoloska@tue.nl, r.m.aarts@tue.nl).

###### Abstract

We propose a categorical approach for unsupervised variational acoustic clustering of audio data in the time-frequency domain. The consideration of a categorical distribution enforces sharper clustering even when data points strongly overlap in time and frequency, which is the case for most datasets of urban acoustic scenes. To this end, we use a Gumbel-Softmax distribution as a soft approximation to the categorical distribution, allowing for training via backpropagation. In this settings, the softmax temperature serves as the main mechanism to tune clustering performance. The results show that the proposed model can obtain impressive clustering performance for all considered datasets, even when data points strongly overlap in time and frequency.

 Clustering algorithms, audio signal processing, unsupervised learning.

## I Introduction

The use of classification and clustering algorithms that allow for specialized processing are crucial for hardware-constrained applications like hearing aids [1, 2]. However, clustering algorithms require labels which can be scarce, as is the case for urban acoustic scenes datasets [3, 4], where they usually represent abstractions, e.g., the place where each audio file was recorded [5]. To optimize specialized processing, we need an unsupervised clustering method based on relevant characteristics of the acoustic signal [6] that is able to process complex and overlapped audio data.

While traditional methods struggle to cluster high-dimensional audio signals [7], variational autoencoders are a feasible option as they can learn without the need for labels [8] and generally do not require a massive number of parameters, as observed for other generative approaches [9]. Within the scope of interest, variational autoencoders were previously applied for unsupervised image clustering [10, 11, 12], and recently for the unsupervised clustering of spoken digits [6].

We build upon [6] by modifying the unsupervised variational acoustic clustering (UVAC) model to a categorical UVAC, inspired by the generative semi-supervised model [13]. The proposed approach employs a categorical distribution, providing efficient clustering performance even for datasets with strongly overlapped data points, as urban acoustic scenes. This is done using a Gumbel-Softmax (GS) distribution [14] for categorical reparametrization, allowing for training via backpropagation, with the softmax temperature serving as a mechanism to tune clustering performance. The results show impressive unsupervised clustering performance for a spoken digits dataset - considered for comparison with the baseline [6] - and for two real-world datasets of urban acoustic scenes.

## II Variational inference

We consider a dataset \(\mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N}\) with \(N\) independent and identically distributed (i.i.d.) samples. Each observation \(\mathbf{x}^{(i)}\) is considered to be generated by a class \(k\) of a categorical latent variable \(y\) and a continuous latent variable \(\mathbf{z}\), similarly to the M2 model in [13]. The joint distribution is given by

\[p_{\theta}(\mathbf{x}^{(\mathbf{i})},y,\mathbf{z})=p_{\theta}(\mathbf{x}^{( \mathbf{i})}|y,\mathbf{z})p_{\theta}(y)p_{\theta}(\mathbf{z}), \tag{1}\]

with model parameters \(\theta\), and \(y\) and \(\mathbf{z}\) independent. We are interested in the true posterior \(p_{\theta}(y,\mathbf{z}|\mathbf{x}^{(i)})\), as it tells us how likely each latent configuration is given an observed data point. From Bayes theorem, we can use the marginal likelihood

\[p_{\theta}(\mathbf{x}^{(i)})=\sum_{y}\int p_{\theta}(\mathbf{x}^{(\mathbf{i}) },y,\mathbf{z})d\mathbf{z} \tag{2}\]

to obtain the true posterior, with parameters \(\theta\) that can be obtained by maximizing the log-likelihood \(\log p_{\theta}(\mathbf{x}^{(i)})\). However, (2) has no closed-form solution for most real-world problems [8].

To solve the intractability of (2), we define a _variational_ distribution with parameters \(\upsilon\) and \(\phi\),

\[q_{\upsilon,\phi}(y,\mathbf{z}|\mathbf{x}^{(i)})=q_{\phi}(\mathbf{z}|\mathbf{ x}^{(i)},y)q_{\upsilon}(y|\mathbf{x}^{(i)}), \tag{3}\]

such that \(q_{\upsilon,\phi}(y,\mathbf{z}|\mathbf{x}^{(i)})\approx p_{\theta}(y,\mathbf{ z}|\mathbf{x}^{(i)})\). Moreover, we rewrite (2) using (3) as

\[\log p_{\theta}(\mathbf{x}^{(i)})=\log\sum_{y}\int q_{\upsilon,\phi}(y, \mathbf{z}|\mathbf{x}^{(i)})\frac{p_{\theta}(\mathbf{x}^{(\mathbf{i})},y, \mathbf{z})}{q_{\upsilon,\phi}(y,\mathbf{z}|\mathbf{x}^{(i)})}d\mathbf{z}, \tag{4}\]

where \(\sum_{y}\int q_{\upsilon,\phi}(y,\mathbf{z}|\mathbf{x}^{(i)})[\cdot]d\mathbf{z}\) represents the expectation over \(q_{\upsilon,\phi}(y,\mathbf{z}|\mathbf{x}^{(i)})\) applied to \([\cdot]\), i.e., \(\mathbb{E}_{q_{\upsilon,\phi}(y,\mathbf{z}|\mathbf{x}^{(i)})}[\cdot]\). Rewriting (4) with the expectation operator gives us

\[\log p_{\theta}(\mathbf{x}^{(i)})=\log\mathbb{E}_{q_{\upsilon,\phi}(y,\mathbf{ z}|\mathbf{x}^{(i)})}\left[\frac{p_{\theta}(\mathbf{x}^{(\mathbf{i})},y, \mathbf{z})}{q_{\upsilon,\phi}(y,\mathbf{z}|\mathbf{x}^{(i)})}\right]. \tag{5}\]

We can apply Jensen's inequality to (5), which yields

\[\log p_{\theta}(\mathbf{x}^{(i)})\geq\mathbb{E}_{q_{\upsilon,\phi}(y,\mathbf{ z}|\mathbf{x}^{(i)})}\left[\log\frac{p_{\theta}(\mathbf{x}^{(\mathbf{i})},y, \mathbf{z})}{q_{\upsilon,\phi}(y,\mathbf{z}|\mathbf{x}^{(i)})}\right], \tag{6}\]whereby, by expanding the rightmost part with (1) and (3), we can define the variational lower bound \(\mathcal{L}^{(i)}(\theta,\upsilon,\phi)\) as

\[\mathcal{L}^{(i)}(\theta,\upsilon,\phi)=\mathbb{E}_{q_{\upsilon, \phi}(\mathbf{y},\mathbf{z}|\mathbf{x}^{(i)})}[\log p_{\theta}(\mathbf{x}^{( \mathbf{i})}|y,\mathbf{z})]\] \[-\lambda(D_{KL}(q_{\phi}(\mathbf{z}|y,\mathbf{x}^{(i)})||p_{ \theta}(\mathbf{z}))+D_{KL}(q_{\upsilon}(y|\mathbf{x}^{(i)})||p_{\theta}(y))). \tag{7}\]

In (7), \(D_{KL}(\cdot||\cdot)\) is the Kullback-Leibler (KL) divergence between two distributions and \(\mathbb{E}_{q_{\upsilon,\phi}(y,\mathbf{z}|\mathbf{x}^{(i)})}[\log p_{\theta}( \mathbf{x}^{(\mathbf{i})}|y,\mathbf{z})]\) is the reconstruction error. Additionally, we include a non-trainable Lagrangian variable \(\lambda\) for smoother training, which should be tuned based on data. The complete model with generation (1) and inference (3) is shown in Figure 1. In this model, each encoder and decoder are represented as a neural network (NN). To sample from a categorical distribution, the model uses a Gumbel-Softmax distribution [14], which we detail in the following.

### _Sampling from a categorical distribution_

The basis of the approach comes from the Gumbel-Max trick [15]. Consider a categorical distribution with class probabilities \([\pi_{1},\...,\pi_{K}]\), defined as

\[y_{1}=\mathrm{one\_hot}\left(\operatorname*{arg\,max}_{k}[\log\pi_{k}+g_{k}] \right)\!, \tag{8}\]

where \(g_{k}\sim-\log(-\log(u))\) are i.i.d. samples drawn from the Gumbel distribution with \(u\sim\mathcal{U}(0,1)\). Thereby, sampling from a discrete distribution is reduced by applying noise to a deterministic function. However, the \(\operatorname*{arg\,max}\) operator is not differentiable. To address this issue, we change the \(\operatorname*{arg\,max}\) operator by its differentiable approximation, the softmax function, and we obtain the Gumbel-Softmax distribution [14]:

\[y=\frac{\exp{((\log\pi_{k}+g_{k})/\tau)}}{\sum_{j=1}^{K}\exp{((\log\pi_{j}+g_{ j})/\tau)}},\quad\text{for }k=1,...,K. \tag{9}\]

The temperature \(\tau\) controls the extent to which the GS approaches a categorical distribution. As \(\tau\to 0\), the continuous GS (9) becomes the categorical Gumbel-Max distribution (8). On the other hand, \(\tau\rightarrow\infty\) makes the GS (9) a uniform distribution. To illustrate, Figure 2 shows the GS distribution plot for 10 classes, with different values of \(\tau\). The softmax temperature \(\tau\) is specially interesting when the GS distribution is used for clustering, as a smaller \(\tau\) results in more distinct and dense clusters. Therefore, as a natural choice, we choose to cluster over \(y\) with a monotonic reduction of \(\tau\) over training.

### _Architecture_

The categorical UVAC is shown in Figure 1. For the discriminative inference encoder \(q_{\upsilon}(y|\mathbf{x}^{(i)})\), we consider a NN denoted by \(h\) with parameters \(\upsilon\) that outputs \(K\) (soft) probabilities, \(h_{\upsilon}(\mathbf{x}^{(i)})=[\pi_{1},\...,\pi_{K}]\). Moreover, the inference encoder \(q_{\phi}(\mathbf{z}|y,\mathbf{x}^{(i)})\) is represented by a NN denoted by \(f\) with parameters \(\phi\) such that \(f_{\phi}(y,\mathbf{x})=[\boldsymbol{\mu}_{\phi},\boldsymbol{\Sigma}_{\phi}]\), which outputs are used for sampling \(\mathbf{z}\) via reparametrization trick [8] as \(\mathbf{z}=\boldsymbol{\mu}_{\phi}+\boldsymbol{\Sigma}_{\phi}^{1/2}\boldsymbol {\epsilon}\), with an auxiliary random variable \(\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\). The generative decoder is given by a NN denoted by \(g\) with parameters \(\theta\) that reconstructs data \(\mathbf{x}^{(i)}\), as \(g_{\theta}(y,\mathbf{z})=[\hat{\mathbf{x}}^{(i)}]\). The NNs follow a similar architecture to that of [6], a state-of-the-art convolutional-recurrent autoencoder approach [16, 17], described as follows.

Both inference networks \(h\) and \(f\) are composed by four 2D convolutional layers followed by two gated recurrent unit (GRU) layers. The convolutions have output channels 16, 32, 64, and 128, while both GRU layers have 128 output nodes. Every convolutional layer is followed by 2D batch normalization and a ReLU activation function. After the last GRU layer, model \(h\) contains a linear layer that outputs \(K\) values, which are used as input to the GS distribution. Differently, after the last GRU layer of \(f\), we use a linear layer to convert the encoder dimension to a latent dimension \(d_{\mathbf{z}}\). The decoder model \(g\) is composed by four 2D transposed convolutional layers with output channels 64, 32, 16, and 1, where every layer is followed by ReLU activation except the last, which uses a sigmoid function. All convolutional kernels are (8,8) with stride (2,2) and padding (3,3). Note that, whenever the input to a NN model is the concatenation of two variables, we concatenate them over the channel dimension, expanding other dimensions as needed.

## III Experimental evaluation

We consider two different tasks for the validation of the proposed method. First, to compare with previous work, we target the unsupervised spoken digit recognition. Second, as a more challenging task, we devise the unsupervised clustering of urban acoustic scenes, composed of real-world background sound recordings. Both cases are described in the following.

Fig. 2: Gumbel-Softmax distribution example plot for 10 classes with different values of \(\tau\).

### _Tasks_

#### Ii-A1 Spoken digit recognition

spoken digit recognition consists of identifying which digit was spoken in an audio utterance. The main acoustic feature is the digit itself, while other sound characteristics are minor features. For such a task, we use the AudioMNIST dataset [18] with 30000 audio samples, being 24000 randomly chosen for training, 3000 for validation, and 3000 for testing. Each file contains the recording of a spoken digit in english. We use the same pre-processing of data as described in [6], feeding a 1-second long magnitude spectrogram with trimmed frequency bins to the NNs.

#### Ii-A2 Urban acoustic scene classification

we devise the classification of urban acoustic scenes using real-world datasets. Such a task is much more challenging as the (background) sound features resemble noise and greatly overlap in time and frequency. We consider two different datasets: TAU2019 [5], with 1200 audio recording files of 10 seconds from different acoustic scenes; and the UrbanSound8K (US8K) [19] dataset, with 8732 labeled sound excerpts of 10 different acoustic scenes, mostly 4-seconds long. Differently from AudioMNIST, we now use a mel-frequency cepstrum to reduce the input dimension without limiting frequency range, as acoustic scene classification can benefit from the broader frequency range.

We resample data to 16 kHz. The US8K audio files are zero-pad to four seconds, where we also employ a voice activity detection mask for the calculation of the reconstruction error, avoiding the network to cluster zero-padding [6]. We obtain a short-term Fourier transform (STFT) of 960 samples with a Hann window the same size and 50% overlap. Lastly, we apply a mel-frequency scale with 128 bins. The cepstrum is normalized by its mean and variance, and a min-max normalization to limit values from 0 to 1. We feed the NNs with a time-context window [20] of 4 seconds for the US8K dataset and 10 seconds for the TAU2019 dataset, which are the maximum duration of the dataset's files.

Differently from AudioMNIST case, urban scene classification has direct application for hearing aids [1, 2], where different acoustic scenes result in different processing, which is proportional to the constraints of the device. We take two cases into account: the same number of clusters as labels in the dataset; and a reduced number of clusters. Specifically for the considered urban acoustic scene datasets, we consider 10 clusters for the higher end, as it matches the number of labels in the dataset. For the lower end, we take 5 clusters into account, as it is a significant reduction from 10, merging similar clusters, but still sufficient for effectively calculating clustering metrics. In practice, a higher number of clusters results in a more complex and "specialized" processing, representing a higher-end version of a hearing aid device. On the other hand, the lower cluster number could represent a more affordable version of the same device.

For this task, we cannot expect high (unsupervised) accuracy from the networks since the audio clips resemble background noise, e.g., TAU2019 have very abstract labels as they tell us where the recordings were made - "airport", "metro station", "tram". Notice that, even when we listen to the TAU2019 audio files, it is difficult to tell which one belongs to each scene. With unsupervised clustering, we separate files based on their main features and statistical behavior, which present a significant benefit for specialized processing when compared to directly using labels such as recording location.

### _Metrics_

Five metrics are considered: i) Unsupervised accuracy - the Hungarian algorithm [21] is used to match cluster labels to truth labels. The matched labels are used for calculating accuracy, ranging from 0 to 100%; ii) Normalized mutual information (NMI) - evaluates how much information is shared between cluster and truth labels [22], ranging from 0 to 1, being proportional to accuracy; iii) Silhouette score - measures how similar a data point is to its cluster in comparison to different clusters [23]. The value reflects on cohesion - how similar data points are in a cluster - and separation - how separate the clusters are. It ranges from - 1 (bad) to 1 (good); iv) Davies-Bouldin index (DBI) - the average similarity rate of each cluster with its most similar cluster [24], being an indication of compactness and separation of clusters. The DBI score ranges from 0 (good) to infinity (bad); v) Cali\(\acute{\text{i}}\)riski-Harabasz index (CHI) - measures the ratio of the sum of between-cluster to within-cluster dispersion [25] - distinctiveness and compactness. It ranges from 0 (bad) to infinity (good). The CHI is added on top of the metrics considered in [6] for a broader analysis. Accuracy and NMI tell us how predicted clusters deviate from labels, and the others show how good the clusters are.

### _Baselines_

As a main baseline, we consider the UVAC model from [6]. We apply it with the exact same architecture as defined in the original paper. We expect UVAC to have a good clustering performance for the better-defined dataset, AudioMNIST, as the main feature - the spoken digit - is free of noise and other overlapping audio content. However, since UVAC takes a Gaussian mixture model (GMM) for clustering, it should fail for noise-resembling data, as the GMM lacks a mechanism to enforce clustering behavior, like the temperature \(\tau\) of the GS. Additionally, we consider a classical approach, the K-means algorithm [26], as a low-complexity baseline.

### _Hyperparameters_

We train the model described in Section II-B for 500 epochs, separately for each considered dataset. All NN parameters \(\theta\), \(\upsilon\), and \(\phi\) are jointly optimized by maximizing (7) with the Adam algorithm [27]. For all cases of the proposed categorical UVAC and the baseline UVAC model [6], the initial learning rate is of \(5\times 10^{-4}\), with exponential decay until the last epoch, reaching \(5\times 10^{-5}\). The Gumbel-Softmax temperature \(\tau\) in (9) of the categorical UVAC is annealed from 1.0 to 0.5 over the epochs, providing a "more uniform" distribution at the beginning of training, avoiding local minima, and converging to an "almost categorical" representation at end of training, enforcing clustering behavior. The value of \(\lambda\) in (7) is chosen experimentally and we try to keep it as low as possible toavoid disturbing the balance of the variational lower bound, while still avoiding the collapse - unintended merging - of clusters. For AudioMNIST, we use \(\tau=0.5\), while we change it to \(\tau=2.0\) for both TAU2019 and UrbanSound8K datasets.

### _Results_

All results are shown in Table I. For the task of spoken digit recognition, the categorical UVAC drastically increases all unsupervised clustering metrics. The Silhouette score, importantly, is almost maxed out, indicating separate and compact clusters. The DBI and CHI indexes also achieve impressive values, reinforcing the hypothesis that the categorical distribution enforces distinct clustering behavior. Specially for AudioMNIST, we also see that the proposed method achieved high accuracy and NMI. Here, the high accuracy and NMI of spoken digits may serve as a "sanity check", indicating that the formed clusters are meaningful. Notice that such behavior is not expected with urban sound datasets since the files resemble background noise. The UVAC method performs the second best, where we can notice similar accuracy and NMI to its categorical version but reduced clustering metrics, showing the limitations of a Gaussian mixture model for clustering. K-means achieves insufficient metrics, only enhancing clustering when compared to the truth labels.

Moreover, the categorical UVAC achieves very high unsupervised clustering metrics for TAU2019 when 10 clusters are considered. As the number of clusters represent the same number as the classes in the dataset, we also investigate the achieved accuracy and NMI. Differently from spoken digit recognition, the urban acoustic scenes strongly overlap in time and frequency, with labels representing an abstraction, indicating where the audio files were recorded. As we can see, the best clustering metrics are achieved with the lowest accuracy and NMI, reinforcing the hypothesis from Section III-A2 - that we cannot obtain high accuracy/NMI and high clustering metrics simultaneously for such datasets. On another hand, the baseline UVAC model is not able to cluster such complex data. This shows that the GMM is insufficient for clustering data that strongly overlaps in time and frequency. K-means achieves similar behavior, failing to cluster as the method is too simple and cannot perform well with most audio data. From the clustering metrics evaluated at the labels, we see that the original classification does not form good clusters, as expected. When the number of cluster is reduced to 5, all methods improve in terms of clustering performance as nearby clusters are merged together. The categorical UVAC still performs better, while the baseline UVAC and K-means are insufficient.

Finally, the UrbanSound8K dataset also presents classes with significant overlap. This is observed once again in the reduced accuracy and NMI but high clustering metrics achieved by the categorical UVAC, showing that even though classes are better defined, their content strongly overlaps. This type of behavior confirms our aforementioned hypothesis for background-noise-like datasets. For all cases, while the baseline UVAC and K-means cannot achieve sufficient clustering, the categorical UVAC achieves high clustering performance, which metrics are increased when the number of clusters is reduced from 10 to 5. In addition, we noticed that clustering performance is dependent on the value of \(\tau\). The relation between softmax temperature and clustering could be explored further, being out of scope for this paper.

## IV Conclusion

We proposed a categorical extension for unsupervised variational acoustic clustering, which clusters audio data in the time-frequency domain. For smooth training, we employed the Gumbel-Softmax distribution, serving as a categorical approximation while still allowing for training via backpropagation. We showed out how the softmax temperature serves as a means to enhance cluster separation and compactness. The proposed approach excels in all considered cases, always achieving very high clustering metrics. As a future work, it would be useful to test the practical applications of categorical UVAC, for example in the speech enhancement of hearing aids.

## V Acknowledgment

The authors would like to thank the insightful comments and revision of Bruno Defraene, Johan David, Alex Young, Yan Wu, Frans Widdershoven, and Wim van Houtum.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline
**Dataset** & **Clusters** & **Method** & **Accuracy (\%) \(\uparrow\)** & **NMI \(\uparrow\)** & **Silhouette \(\uparrow\)** & **DBI \(\downarrow\)** & **CHI \(\times 10^{3}\uparrow\)** \\ \hline \multirow{4}{*}{AudioMNIST} & \multirow{4}{*}{10} & None (labels)* & \(100.00\) & \(1.00\) & -0.04 & 5.56 & 0.10 \\  & & K-means* & 18.40 & 0.10 & 0.13 & 2.04 & 0.69 \\  & & UVAC* & 70.78 & 0.71 & 0.21 & 1.61 & 0.54 \\  & & Cat. UVAC (\(\lambda=0.5\)) & 76.30 & 0.78 & **0.97** & **0.07** & **40.14** \\ \hline \multirow{4}{*}{TAU2019} & \multirow{4}{*}{10} & None (labels) & \(100.00\) & \(1.00\) & -0.05 & 9.17 & 0.11 \\  & & K-means & 25.20 & 0.19 & 0.03 & 3.48 & 0.28 \\  & & UVAC & 28.73 & 0.17 & -0.02 & 5.07 & 0.25 \\  & & Cat. UVAC (\(\lambda=2.0\)) & 23.63 & 0.15 & **0.77** & **0.33** & **6.97** \\ \cline{2-8}  & \multirow{4}{*}{5} & K-means & – & – & 0.06 & 3.12 & 0.53 \\  & & UVAC & – & – & 0.02 & 3.41 & 0.34 \\  & & Cat. UVAC (\(\lambda=2.0\)) & – & – & **0.79** & **0.30** & **14.76** \\ \hline \multirow{4}{*}{UrbanSound8K} & \multirow{4}{*}{10} & None (labels) & \(100.00\) & \(1.00\) & -0.06 & 4.90 & 0.04 \\  & & K-means & 34.17 & 0.30 & 0.14 & 1.90 & 0.20 \\ \cline{1-1}  & & UVAC & 35.46 & 0.30 & 0.09 & 2.35 & 0.11 \\ \cline{1-1}  & & Cat. UVAC (\(\lambda=2.0\)) & 25.52 & 0.15 & **0.73** & **0.39** & **1.17** \\ \cline{1-1} \cline{2-8}  & \multirow{4}{*}{5} & K-means & – & – & 0.18 & 1.63 & 0.32 \\ \cline{1-1}  & & UVAC & – & – & 0.10 & 2.33 & 0.14 \\ \cline{1-1}  & & Cat. UVAC (\(\lambda=2.0\)) & – & – & **0.78** & **0.32** & **3.19** \\ \hline \hline \end{tabular}
\end{table} TABLE I: Clustering metrics on the test set of the mentioned datasets, either considering labels as clusters or by applying K-means, UVAC and categorical (Cat.) UVAC as clustering methods, averaged over 10 independent runs. *results from [6].

## References

* [1] S. Yook, K. W. Nam, H. Kim, S. H. Hong, D. P. Jang, and I. Y. Kim, "An environment-adaptive management algorithm for hearing-support devices incorporating listening situation and noise type classifiers," _Artificial Organs_, vol. 39, no. 4, pp. 361-368, April 2015.
* [2] L. Lamarche, C. Giguere, W. Gueaich, T. Aboulnastr, and H. Othman, "Adaptive environment classification system for hearing aids," _The Journal of the Acoustical Society of America_, vol. 127, no. 5, pp. 3124-3135, May 2010.
* [3] L. V. Fiorio, B. Karanov, J. David, W. v. Houtum, F. Widdershoven, and R. M. Aarts, "Semi-supervised learning with per-class adaptive confidence scores for acoustic environment classification with imbalanced data," in _2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, 2023, pp. 1-5.
* [4] W. Han, E. Coutinho, H. Ruan, H. Li, B. Schuller, X. Yu, and X. Zhu, "Semi-supervised active learning for sound classification in hybrid learning environments," _PLOS ONE_, vol. 11, no. 9, pp. 1-23, 09 2016.
* [5] A. Mesaros, T. Heittola, and T. Virtanen, "A multi-device dataset for urban acoustic scene classification," in _Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)_, November 2018, pp. 9-13.
* [6] L. V. Fiorio, B. Defranee, J. David, F. Widdershoven, W. van Houtum, and R. M. Aarts, "Unsupervised Variational Acoustic Clustering," 2025. [Online]. Available: [https://arxiv.org/abs/2503.18579](https://arxiv.org/abs/2503.18579)
* [7] J. Foote, "An overview of audio information retrieval," _Multimedia Syst._, vol. 7, no. 1, p. 2-10, Jan. 1999.
* [8] D. P. Kingma and M. Welling, "Auto-Encoding Variational Bayes," in _2nd International Conference on Learning Representations, ICLR 2014_, 2014.
* [9] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, "Scaling Laws for Neural Language Models," 2020.
* [10] Y. Ugur, G. Arvanitakis, and A. Zaidi, "Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding," _Entropy_, vol. 22, no. 2, 2020.
* [11] Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou, "Variational deep embedding: an unsupervised and generative approach to clustering," in _Proceedings of the 26th International Joint Conference on Artificial Intelligence, IJCAI 2017_, 2017, pp. 1965-1972.
* [12] N. Djolckthanakul, P. A. M. Medrano, M. Garnelo, M. C. H. Lee, H. Salimbeni, K. Arulkumaran, and M. Shanahan, "Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders," 2017.
* [13] D. P. Kingma, D. J. Rezende, S. Mohamed, and M. Welling, "Semi-supervised Learning with Deep Generative Models," in _Advances in Neural Information Processing Systems_, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, Eds., vol. 27. Curran Associates, Inc., 2014.
* [14] E. Jang, S. Gu, and B. Poole, "Categorical Reparameterization with Gumbel-Softmax," in _International Conference on Learning Representations (ICLR)_, 2017.
* [15] C. J. Maddison, D. Tarlow, and T. Minka, "A* sampling," 2015. [Online]. Available: [https://arxiv.org/abs/1411.0030](https://arxiv.org/abs/1411.0030)
* [16] D. O'Sughangnessy, "Speech Enhancement--A Review of Modern Methods," _IEEE Transactions on Human-Machine Systems_, vol. 54, no. 1, pp. 110-120, 2024.
* [17] H. Zhao, S. Zarar, I. Tashev, and C.-H. Lee, "Convolutional-recurrent neural networks for speech enhancement," in _2018 IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)_, April 2018.
* [18] S. Becker, J. Vielhaben, M. Ackermann, K.-R. Muller, S. Lapuschkin, and W. Samek, "AudioMNIST: Exploring Explainable Artificial Intelligence for audio analysis on a simple benchmark," _Journal of the Franklin Institute_, 2023.
* [19] J. Salamon, C. Jacoby, and J. P. Bello, "A dataset and taxonomy for urban sound research," in _Proceedings of the 22nd ACM International Conference on Multimedia_, New York, NY, USA: Association for Computing Machinery, 2014, p. 1041-1044.
* [20] L. V. Fiorio, B. Karanov, B. Defranee, J. David, F. Widdershoven, W. Van Houtum, and R. M. Aarts, "Spectral Masking With Explicit Time-Context Withoraw for Neural Network-Based Monaural Speech Enhancement," _IEEE Access_, vol. 12, pp. 154 843-154 852, 2024.
* [21] H. W. Kuhn, "The Hungarian method for the assignment problem," _Naval Research Logistics Quarterly_, vol. 2, no. 1-2, pp. 83-97, 1955.
* [22] N. X. Vinh, J. Epps, and J. Bailey, "Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance," _J. Mach. Learn. Res._, vol. 11, p. 2837-2854, Dec. 2010.
* [23] P. J. Rousseeuw, "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis," _Journal of Computational and Applied Mathematics_, vol. 20, pp. 53-65, 1987.
* [24] D. L. Davies and D. W. Bouldin, "A cluster separation measure," _IEEE Transactions on Pattern Analysis and Machine Intelligence_, vol. PAMI-1, no. 2, pp. 224-227, 1979.
* [25] T. Calinski and J. Harabasz, "A dendrite method for cluster analysis," _Communications in Statistics_, vol. 3, no. 1, pp. 1-27, 1974.
* [26] J. A. Harigan and M. A. Wong, "Algorithm AS 136: A K-Means Clustering Algorithm," _Journal of the Royal Statistical Society. Series C (Applied Statistics)_, vol. 28, no. 1, pp. 100-108, 1979.
* [27] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," in _3rd International Conference on Learning Representations (ICLR)_, 2015.